{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock - End to end example\n",
    "\n",
    "This notebook provides sample code for building an empty OpenSearch Serverless (OSS) index, Amazon Bedrock knowledge base and ingest documents into the index.\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "A data pipeline that ingests documents (typically stored in Amazon S3) into a knowledge base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "- Load the documents into the knowledge base by connecting your s3 bucket (data source). \n",
    "- Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Steps: \n",
    "- Create Amazon Bedrock Knowledge Base execution role with necessary policies for accessing data from S3 and writing embeddings into OSS.\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Download documents\n",
    "- Create Amazon Bedrock knowledge base\n",
    "- Create a data source within knowledge base which will connect to Amazon S3\n",
    "- Start an ingestion job using KB APIs which will read data from s3, chunk it, convert chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy and manage the data pipeline.\n",
    "\n",
    "Once the data is available in the Bedrock Knowledge Base then a question answering application can be built using the Knowledge Base APIs provided by Amazon Bedrock in following notebooks in the same folder. \n",
    "- [customized-rag-retrieve-api-langchain-claude-v3.ipynb](./customized-rag-retrieve-api-langchain-claude-v3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "- create and delete Amazon IAM roles\n",
    "- create, update and delete Amazon S3 buckets\n",
    "- access Amazon Bedrock\n",
    "- access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "- IAMFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to enable `Anthropic Claude 3 Sonnet`  and `Anthropic Claude 3 Haiku` model access in Amazon Bedrock Console, as the notebook will use Anthropic Claude 3 Sonnet and Claude 3 Haiku models for testing the knowledge base once its created.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensearch-py==2.3.1\n",
      "  Downloading opensearch_py-2.3.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting urllib3<2,>=1.21.1 (from opensearch-py==2.3.1)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /Users/praghavs/.venv/lib/python3.12/site-packages (from opensearch-py==2.3.1) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/praghavs/.venv/lib/python3.12/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/praghavs/.venv/lib/python3.12/site-packages (from opensearch-py==2.3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /Users/praghavs/.venv/lib/python3.12/site-packages (from opensearch-py==2.3.1) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/praghavs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/praghavs/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.10)\n",
      "Downloading opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Installing collected packages: urllib3, opensearch-py\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: opensearch-py\n",
      "    Found existing installation: opensearch-py 2.7.1\n",
      "    Uninstalling opensearch-py-2.7.1:\n",
      "      Successfully uninstalled opensearch-py-2.7.1\n",
      "Successfully installed opensearch-py-2.3.1 urllib3-1.26.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting retrying==1.3.4\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/praghavs/.venv/lib/python3.12/site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: retrying\n",
      "Successfully installed retrying-1.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opensearch-py==2.3.1\n",
    "# %pip install -U boto3==1.33.2\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-153264177053 Exists\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f'Bucket {bucket_name} Exists')\n",
    "except ClientError as e:\n",
    "    print(f'Creating bucket {bucket_name}')\n",
    "    if region_name == \"us-east-1\":\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name)\n",
    "    else:\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "First of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Mon, 23 Sep 2024 09:50:57 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '96950332-413e-4745-8635-de20d415c348'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '96950332-413e-4745-8635-de20d415c348',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:153264177053:collection/bizwl17bluxexgdaziz7',\n",
      "                              'createdDate': 1727085057568,\n",
      "                              'id': 'bizwl17bluxexgdaziz7',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1727085057568,\n",
      "                              'name': 'bedrock-sample-rag-312',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'encryption_policy' (dict)\n",
      "Stored 'network_policy' (dict)\n",
      "Stored 'access_policy' (dict)\n",
      "Stored 'collection' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store encryption_policy network_policy access_policy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bizwl17bluxexgdaziz7.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:153264177053:collection/bizwl17bluxexgdaziz7',\n",
      "    'collectionEndpoint': 'https://bizwl17bluxexgdaziz7.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1727085057568,\n",
      "    'dashboardEndpoint': 'https://bizwl17bluxexgdaziz7.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': 'bizwl17bluxexgdaziz7',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1727085081134,\n",
      "    'name': 'bedrock-sample-rag-312',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# wait for collection creation\n",
    "# This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::153264177053:policy/AmazonBedrockOSSPolicyForKnowledgeBase_473\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-312',\n",
      "  'shards_acknowledged': True}\n",
      "............................................................\r"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to ingest into our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 Bucket data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Ingest strategy - How to ingest data from the data source\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 9, 23, 10, 1, 36, 437163, tzinfo=tzutc()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:153264177053:knowledge-base/CET723VMXX',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'CET723VMXX',\n",
      "  'name': 'bedrock-sample-knowledge-base-312',\n",
      "  'roleArn': 'arn:aws:iam::153264177053:role/AmazonBedrockExecutionRoleForKnowledgeBase_473',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:153264177053:collection/bizwl17bluxexgdaziz7',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-312'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 9, 23, 10, 1, 36, 437163, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 9, 23, 10, 1, 58, 403222, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-153264177053'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'IVD2HH00QW',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'CET723VMXX',\n",
      "  'name': 'bedrock-sample-knowledge-base-312',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 9, 23, 10, 1, 58, 403222, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd6e006fe-1f8e-444a-8e20-170dc6326a64',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 23 Sep 2024 10:02:06 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '603',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd6e006fe-1f8e-444a-8e20-170dc6326a64',\n",
       "   'x-amz-apigw-id': 'ejZouFoLIAMEoIA=',\n",
       "   'x-amzn-trace-id': 'Root=1-66f13c9d-3873262b2b44d81a487b487c'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'createdAt': datetime.datetime(2024, 9, 23, 10, 1, 58, 403222, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-153264177053'},\n",
       "   'type': 'S3'},\n",
       "  'dataSourceId': 'IVD2HH00QW',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'knowledgeBaseId': 'CET723VMXX',\n",
       "  'name': 'bedrock-sample-knowledge-base-312',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2024, 9, 23, 10, 1, 58, 403222, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\r"
     ]
    }
   ],
   "source": [
    "# Start an ingestion job\n",
    "interactive_sleep(30)\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'IVD2HH00QW',\n",
      "  'ingestionJobId': 'VSQ7I9QHY3',\n",
      "  'knowledgeBaseId': 'CET723VMXX',\n",
      "  'startedAt': datetime.datetime(2024, 9, 23, 10, 2, 52, 599598, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 0,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 0},\n",
      "  'status': 'STARTING',\n",
      "  'updatedAt': datetime.datetime(2024, 9, 23, 10, 2, 52, 599598, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dataSourceId': 'IVD2HH00QW',\n",
      "  'ingestionJobId': 'VSQ7I9QHY3',\n",
      "  'knowledgeBaseId': 'CET723VMXX',\n",
      "  'startedAt': datetime.datetime(2024, 9, 23, 10, 2, 52, 599598, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 9, 23, 10, 3, 7, 213526, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    \n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CET723VMXX'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Note: If you plan to run any following notebooks, you can skip this section\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic Claude 3 models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon has been working on developing its own large language models (LLMs) '\n",
      " 'for generative AI applications. The company believes generative AI will '\n",
      " 'transform and improve virtually every customer experience across its '\n",
      " 'consumer, seller, brand, and creator offerings. Amazon is investing '\n",
      " 'substantially in LLMs and plans to continue doing so. On the AWS cloud '\n",
      " 'platform, Amazon is democratizing generative AI technology by offering '\n",
      " 'machine learning chips like Trainium and Inferentia that provide '\n",
      " 'cost-effective training and running of LLMs. AWS also enables companies to '\n",
      " \"choose from various LLMs and build applications using AWS's security, \"\n",
      " \"privacy and other features. One example is AWS's CodeWhisperer, which uses \"\n",
      " 'generative AI to provide real-time code suggestions to improve developer '\n",
      " 'productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.',\n",
      "  'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n",
      "---------- Generated using Claude 3 Haiku:\n",
      "('Amazon has been investing heavily in large language models (LLMs) and '\n",
      " 'generative AI, which they believe will significantly accelerate the adoption '\n",
      " 'of machine learning across their various businesses. They have been working '\n",
      " 'on developing their own LLMs, which they believe will transform and improve '\n",
      " 'virtually every customer experience at Amazon. Additionally, Amazon is '\n",
      " 'democratizing this technology through AWS, offering the most '\n",
      " 'price-performant machine learning chips in Trainium and Inferentia so that '\n",
      " 'companies of all sizes can afford to train and run their LLMs in production. '\n",
      " 'AWS is also enabling companies to choose from various LLMs and build '\n",
      " \"applications with AWS's security, privacy, and other features.\")\n",
      "---------- The citations for the response generated by Claude 3 Haiku:\n",
      "[ 'Amazon has been using machine learning extensively for 25 years, employing '\n",
      "  'it in everything from personalized ecommerce recommendations, to '\n",
      "  'fulfillment center pick paths, to drones for Prime Air, to Alexa, to the '\n",
      "  'many machine learning services AWS offers (where AWS has the broadest '\n",
      "  'machine learning functionality and customer base of any cloud provider). '\n",
      "  'More recently, a newer form of machine learning, called Generative AI, has '\n",
      "  'burst onto the scene and promises to significantly accelerate machine '\n",
      "  'learning adoption. Generative AI is based on very Large Language Models '\n",
      "  '(trained on up to hundreds of billions of parameters, and growing), across '\n",
      "  'expansive datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%). And, it’s a similar story for '\n",
      "  'Global IT spending, where we have AWS revenue of $80B in 2022, with about '\n",
      "  '90% of Global IT spending still on-premises and yet to migrate to the '\n",
      "  'cloud.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ { 'content': { 'text': 'Amazon has been using machine learning extensively '\n",
      "                         'for 25 years, employing it in everything from '\n",
      "                         'personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption. Generative AI '\n",
      "                         'is based on very Large Language Models (trained on '\n",
      "                         'up to hundreds of billions of parameters, and '\n",
      "                         'growing), across expansive datasets, and has '\n",
      "                         'radically general and broad recall and learning '\n",
      "                         'capabilities. We have been working on our own LLMs '\n",
      "                         'for a while now, believe it will transform and '\n",
      "                         'improve virtually every customer experience, and '\n",
      "                         'will continue to invest substantially in these '\n",
      "                         'models across all of our consumer, seller, brand, '\n",
      "                         'and creator experiences. Additionally, as we’ve done '\n",
      "                         'for years in AWS, we’re democratizing this '\n",
      "                         'technology so companies of all sizes can leverage '\n",
      "                         'Generative AI. AWS is offering the most '\n",
      "                         'price-performant machine learning chips in Trainium '\n",
      "                         'and Inferentia so small and large companies can '\n",
      "                         'afford to train and run their LLMs in production. We '\n",
      "                         'enable companies to choose from various LLMs and '\n",
      "                         'build applications with all of the AWS security, '\n",
      "                         'privacy and other features that customers are '\n",
      "                         'accustomed to using. And, we’re delivering '\n",
      "                         'applications like AWS’s CodeWhisperer, which '\n",
      "                         'revolutionizes        developer productivity by '\n",
      "                         'generating code suggestions in real time. I could '\n",
      "                         'write an entire letter on LLMs and Generative AI as '\n",
      "                         'I think they will be that transformative, but I’ll '\n",
      "                         'leave that for a future letter. Let’s just say that '\n",
      "                         'LLMs and Generative AI are going to be a big deal '\n",
      "                         'for customers, our shareholders, and Amazon.   So, '\n",
      "                         'in closing, I’m optimistic that we’ll emerge from '\n",
      "                         'this challenging macroeconomic time in a stronger '\n",
      "                         'position than when we entered it. There are several '\n",
      "                         'reasons for it and I’ve mentioned many of them '\n",
      "                         'above. But, there are two relatively simple '\n",
      "                         'statistics that underline our immense future '\n",
      "                         'opportunity. While we have a consumer business '\n",
      "                         'that’s $434B in 2022, the vast majority of total '\n",
      "                         'market segment share in global retail still resides '\n",
      "                         'in physical stores (roughly 80%). And, it’s a '\n",
      "                         'similar story for Global IT spending, where we have '\n",
      "                         'AWS revenue of $80B in 2022, with about 90% of '\n",
      "                         'Global IT spending still on-premises and yet to '\n",
      "                         'migrate to the cloud.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3A2u5VHpIBnk0M5gezq8_A',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'IVD2HH00QW',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.6664225},\n",
      "  { 'content': { 'text': 'Our Inferentia2 chip, which just launched, offers up '\n",
      "                         'to four times higher throughput and ten times lower '\n",
      "                         'latency than our first Inferentia processor. With '\n",
      "                         'the enormous upcoming growth in machine learning, '\n",
      "                         'customers will be able to get a lot more done with '\n",
      "                         'AWS’s training and inference chips at a '\n",
      "                         'significantly lower cost. We’re not close to being '\n",
      "                         'done innovating here, and this long-term investment '\n",
      "                         'should prove fruitful for both customers and AWS. '\n",
      "                         'AWS is still in the early stages of its evolution, '\n",
      "                         'and has a chance for unusual growth in the next '\n",
      "                         'decade.   Similarly high potential, Amazon’s '\n",
      "                         'Advertising business is uniquely effective for '\n",
      "                         'brands, which is part of why it continues to grow at '\n",
      "                         'a brisk clip. Akin to physical retailers’ '\n",
      "                         'advertising businesses selling shelf space, end- '\n",
      "                         'caps, and placement in their circulars, our '\n",
      "                         'sponsored products and brands offerings have been an '\n",
      "                         'integral part        of the Amazon shopping '\n",
      "                         'experience for more than a decade. However, unlike '\n",
      "                         'physical retailers, Amazon can tailor these '\n",
      "                         'sponsored products to be relevant to what customers '\n",
      "                         'are searching for given what we know about shopping '\n",
      "                         'behaviors and our very deep investment in machine '\n",
      "                         'learning algorithms. This leads to advertising '\n",
      "                         'that’s more useful for customers; and as a result, '\n",
      "                         'performs better for brands. This is part of why our '\n",
      "                         'Advertising revenue has continued to grow rapidly '\n",
      "                         '(23% YoY in Q4 2022, 25% YoY overall for 2022 on a '\n",
      "                         '$31B revenue base), even as most large '\n",
      "                         'advertising-focused businesses’ growth have slowed '\n",
      "                         'over the last several quarters.   We strive to be '\n",
      "                         'the best place for advertisers to build their '\n",
      "                         'brands. We have near and long-term opportunities '\n",
      "                         'that will help us achieve that mission. We’re '\n",
      "                         'continuing to make large investments in machine '\n",
      "                         'learning to keep honing our advertising selection '\n",
      "                         'algorithms. For the past couple of years, we’ve '\n",
      "                         'invested in building comprehensive, flexible, and '\n",
      "                         'durable planning and measurement solutions, giving '\n",
      "                         'marketers greater insight into advertising '\n",
      "                         'effectiveness. An example is Amazon Marketing Cloud '\n",
      "                         '(“AMC”). AMC is a “clean room” (i.e. secure digital '\n",
      "                         'environment) in which advertisers can run custom '\n",
      "                         'audience and campaign analytics across a range of '\n",
      "                         'first and third-party inputs, in a privacy-safe '\n",
      "                         'manner, to generate advertising and business '\n",
      "                         'insights to inform their broader marketing and sales '\n",
      "                         'strategies. The Advertising and AWS teams have '\n",
      "                         'collaborated to enable companies to store their data '\n",
      "                         'in AWS, operate securely in AMC with Amazon and '\n",
      "                         'other third-party data sources, perform analytics in '\n",
      "                         'AWS, and have the option to activate advertising on '\n",
      "                         'Amazon or third-party publishers through the Amazon '\n",
      "                         'Demand-Side Platform.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3A0-5VHpIBnk0M5gezq8_A',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'IVD2HH00QW',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.5487171},\n",
      "  { 'content': { 'text': 'Imagine what they’ll be able to do with reliable '\n",
      "                         'connectivity, from people taking online education '\n",
      "                         'courses, using financial services, starting their '\n",
      "                         'own businesses, doing their shopping, enjoying '\n",
      "                         'entertainment, to businesses and governments '\n",
      "                         'improving their coverage, efficiency, and '\n",
      "                         'operations. Kuiper will deliver not only '\n",
      "                         'accessibility, but affordability. Our teams have '\n",
      "                         'developed low-cost antennas (i.e. customer '\n",
      "                         'terminals) that will lower the barriers to access. '\n",
      "                         'We recently unveiled the new terminals that will '\n",
      "                         'communicate with the satellites passing overhead, '\n",
      "                         'and we expect to be able to produce our standard '\n",
      "                         'residential version for less than $400 each. They’re '\n",
      "                         'small: 11 inches square, 1 inch thick, and weigh '\n",
      "                         'less than 5 pounds without their mounting bracket, '\n",
      "                         'but they deliver speeds up to 400 megabits per '\n",
      "                         'second. And they’re powered by Amazon-designed '\n",
      "                         'baseband chips. We’re preparing to launch two '\n",
      "                         'prototype satellites to test the entire end-to-end '\n",
      "                         'communications network this year, and plan to be in '\n",
      "                         'beta with commercial customers in 2024. The customer '\n",
      "                         'reaction to what we’ve shared thus far about Kuiper '\n",
      "                         'has been very positive, and we believe Kuiper '\n",
      "                         'represents a very large potential opportunity for '\n",
      "                         'Amazon. It also shares several similarities to AWS '\n",
      "                         'in that it’s capital intensive at the start, but has '\n",
      "                         'a large prospective consumer, enterprise, and '\n",
      "                         'government customer base, significant revenue and '\n",
      "                         'operating profit potential, and relatively few '\n",
      "                         'companies with the technical and inventive aptitude, '\n",
      "                         'as well as the investment hypothesis to go after '\n",
      "                         'it.   One final investment area that I’ll mention, '\n",
      "                         'that’s core to setting Amazon up to invent in every '\n",
      "                         'area of our business for many decades to come, and '\n",
      "                         'where we’re investing heavily is Large Language '\n",
      "                         'Models (“LLMs”) and Generative AI. Machine learning '\n",
      "                         'has been a technology with high promise for several '\n",
      "                         'decades, but it’s only been the last five to ten '\n",
      "                         'years that it’s started to be used more pervasively '\n",
      "                         'by companies. This shift was driven by several '\n",
      "                         'factors, including access to higher volumes of '\n",
      "                         'compute capacity at lower prices than was ever '\n",
      "                         'available. Amazon has been using machine learning '\n",
      "                         'extensively for 25 years, employing it in everything '\n",
      "                         'from personalized ecommerce recommendations, to '\n",
      "                         'fulfillment center pick paths, to drones for Prime '\n",
      "                         'Air, to Alexa, to the many machine learning services '\n",
      "                         'AWS offers (where AWS has the broadest machine '\n",
      "                         'learning functionality and customer base of any '\n",
      "                         'cloud provider). More recently, a newer form of '\n",
      "                         'machine learning, called Generative AI, has burst '\n",
      "                         'onto the scene and promises to significantly '\n",
      "                         'accelerate machine learning adoption.'},\n",
      "    'location': { 's3Location': { 'uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "                  'type': 'S3'},\n",
      "    'metadata': { 'x-amz-bedrock-kb-chunk-id': '1%3A0%3A2e5VHpIBnk0M5gezq8_A',\n",
      "                  'x-amz-bedrock-kb-data-source-id': 'IVD2HH00QW',\n",
      "                  'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-153264177053/AMZN-2022-Shareholder-Letter.pdf'},\n",
      "    'score': 0.53047615}]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Next steps:</b> Proceed to the next labs to learn how to use Bedrock Knowledge bases. Remember to CLEAN_UP at the end of your session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
